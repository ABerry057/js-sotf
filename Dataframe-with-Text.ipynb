{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program begins with the jstor data and creates a dataframe that is used for topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk import ne_chunk_sents, ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citations(doc_type, doc_root):\n",
    "    if doc_type != 'research-article':\n",
    "        return []\n",
    "    xml_cits = doc_root.findall('back/fn-group/fn/p/mixed-citation')\n",
    "    citations = []\n",
    "    for i in range(len(xml_cits)):\n",
    "        try:\n",
    "            cit_author = xml_cits[i].find('person-group/string-name/surname').text\n",
    "        except AttributeError:\n",
    "            cit_author = ''\n",
    "        try:\n",
    "            cit_title = xml_cits[i].find('source').text\n",
    "        except AttributeError:\n",
    "            cit_title = ''\n",
    "        try:\n",
    "            cit_year = xml_cits[i].find('year').text\n",
    "        except AttributeError:\n",
    "            cit_year = ''\n",
    "        try:\n",
    "            cit_reference = xml_cits[i].text\n",
    "        except AttributeError:\n",
    "            cit_reference = ''\n",
    "        source = (cit_author, cit_title, cit_year, cit_reference)\n",
    "        citations.append(source)\n",
    "    return citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml2csv(src_path):\n",
    "    \"\"\"Creates an initial dataset from XML files found in src_path. Columns of the CSV include\n",
    "    id, author, title, year, type, and language. Returns a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        src_path (String): path to directory of XML files to pull metadata from\n",
    "    \"\"\"\n",
    "    src_path = Path(src_path).resolve()\n",
    "    files = src_path.iterdir()\n",
    "    cols = ['id', 'type', 'title', 'auth1', 'year', 'lang','citations']\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    for i, f in tqdm(enumerate(files), desc='Reading metadata files'):    \n",
    "        tree = ET.parse(f)\n",
    "        root = tree.getroot()\n",
    "        id = str(f).split(\"metadata/\")[0].split(\".x\")\n",
    "        type = root.attrib['article-type']\n",
    "        # title handling\n",
    "        title_group = root.find('front/article-meta/title-group')\n",
    "        if title_group is not None and len(title_group.getchildren()) > 0:\n",
    "            title = list(title_group.itertext())[1]\n",
    "        else:\n",
    "            title = ''\n",
    "        # author handling\n",
    "        contrib_group = root.find('front/article-meta/contrib-group')\n",
    "        if contrib_group is not None and len(contrib_group.getchildren()) > 0:\n",
    "            auth1 = ' '.join([list(c.itertext())[0] for c in root.find('front/article-meta/contrib-group/contrib/string-name')])\n",
    "        else:\n",
    "            auth1 = ''\n",
    "        lang = list(root.find('front/article-meta/custom-meta-group/custom-meta/meta-value').itertext())[0]\n",
    "        year = int(list(root.find('front/article-meta/pub-date/year').itertext())[0])\n",
    "        # citation handling\n",
    "        citations = get_citations(type, root)\n",
    "        df.loc[i] = [id, type, title, auth1, year, lang, citations]\n",
    "    print(f\"\\nCollected {df.shape[0]} articles\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean auth1 values by splitting merged names\n",
    "def format_names(name):\n",
    "    \"\"\"Splits merged strings representing author names into forename and surname.\n",
    "    Does not modify correctly formatted names.\n",
    "\n",
    "    Arguments:\n",
    "        name {String} -- Merged fore and surnames\n",
    "    \"\"\"\n",
    "    n_caps = len(re.findall('[A-Z]', name))\n",
    "    n_spaces = len(re.findall(' ', name))\n",
    "    if any(\"\\u0590\" <= c <= \"\\u05EA\" for c in name):\n",
    "        # pass formatting for non-English names\n",
    "        return name\n",
    "    if n_caps - n_spaces != 1:\n",
    "        comps = re.findall('[A-Z][^A-Z]*', name)\n",
    "        # remove whitespace before or after components\n",
    "        comps = [c.strip() for c in comps]\n",
    "        f_name = \" \".join(comps).replace(\"- \", \"-\").replace(\"I \", \"I\")\n",
    "        return f_name\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_misc_articles(df):\n",
    "    \"\"\"Removes articles with the type 'misc' and stores them in a\n",
    "    separate dataframe. Returns a tuple of the misc dataframe\n",
    "    and a copy of df with the misc article rows removed.\n",
    "\n",
    "    Args:\n",
    "        df (Pandas dataframe): Dataframe from which to remove misc rows\n",
    "\n",
    "    Returns:\n",
    "        [Tuple]: (misc dataframe, copy of original dataframe with misc removed)\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "    misc_indices = df[df['type'] == 'misc'].index\n",
    "    misc_df = df.loc[misc_indices]\n",
    "    clean_df.drop(misc_indices, axis=0, inplace=True)\n",
    "    return (clean_df, misc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text (df1):\n",
    "    ocr_dir='DIRECTORY WITH OCR TEXT FILES'\n",
    "    for i in range (1,len(df1)):\n",
    "        df1.loc[i,'id'][0]=df1.loc[i,'id'][0].replace('metadata','ocr')\n",
    "        text_id=df1.loc[i,'id'][0]+'.txt'\n",
    "        with open (text_id,'r',encoding='utf8') as infile:\n",
    "            f=infile.read()\n",
    "            df1.loc[i,'text']=f\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refdf=xml2csv('DIRECTORY WITH DFR METADATA FILES')\n",
    "refdf['auth1'].apply(format_names)\n",
    "ref_df=remove_misc_articles(refdf)\n",
    "ref_df1=ref_df[0]\n",
    "ref_df1=ref_df1.reset_index()\n",
    "ref_df1['text']=''\n",
    "reffinal_df=add_text(ref_df1)\n",
    "\n",
    "reffinal_df.to_csv('referenceDF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reffinal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reffinal_df['text'] = [''.join(x.strip().split('**********')) for x in reffinal_df['text']]\n",
    "reffinal_df['text'] = [' '.join(x.split('_______')) for x in reffinal_df['text']]\n",
    "reffinal_df['text'] = [''.join(x.split('\\n                    ')) for x in reffinal_df['text']]\n",
    "reffinal_df['text'] = [' '.join(x.split('         ')) for x in reffinal_df['text']]\n",
    "reffinal_df['text'] = [' '.join(x.split('<plain_text>')) for x in reffinal_df['text']]\n",
    "reffinal_df['text'] = [' '.join(x.split('</plain_text>')) for x in reffinal_df['text']]\n",
    "reffinal_df['text'].replace('[^A-Za-z0-9]+',' ',regex=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_chunks(named_entities,text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "            else:\n",
    "                continue\n",
    "    named_entities += continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "named_entities = []\n",
    "article = 0\n",
    "for a in reffinal_df['text']:\n",
    "    get_continuous_chunks(named_entities,a)\n",
    "    article = a\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_of_named_entities.pickle', 'wb') as file:\n",
    "    pickle.dump(named_entities, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities_counts = Counter(named_entities)\n",
    "len(named_entities_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities_counts = sorted(named_entities_counts.items(), key=operator.itemgetter(1),reverse=True)\n",
    " \n",
    "with open('dict_of_named_entities_counts.pickle', 'wb') as file:\n",
    "    pickle.dump(named_entities_counts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final list of 1000 most occurring named entities to remove from text\n",
    "common_entities = []\n",
    "for i in np.arange(0,1000):\n",
    "    common_entities.append(\n",
    "#         [\n",
    "            named_entities_counts[i][0]\n",
    "#                                ,named_entities_counts[i][1]]\n",
    "    )\n",
    "common_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy output above and paste into brackets, and manually remove any words you want to retain in the text\n",
    "entities_to_remove=[\n",
    "    'YOUR WORDS HERE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_to_remove=sorted(entities_to_remove)\n",
    "entities_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities_to_remove.pickle', 'wb') as file:\n",
    "    pickle.dump(entities_to_remove, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entities(article):\n",
    "    for entity in entities_to_remove:\n",
    "        if ' '+entity+' ' in article:\n",
    "            article = article.replace(entity+' ','') \n",
    "        elif ' '+entity+'.' in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+',' in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+':' in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+'-' in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+';' in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+'\"' in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+\"'\" in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+\"]\" in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+\")\" in article: # added later\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+\"?\" in article:\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif ' '+entity+\"!\" in article: # added later\n",
    "            article = article.replace(' '+entity,'')\n",
    "        elif '\"'+entity+' ' in article:\n",
    "            article = article.replace(entity+' ','')\n",
    "        elif \"'\"+entity+' ' in article:\n",
    "            article = article.replace(entity+' ','')\n",
    "        elif \"[\"+entity+' ' in article:\n",
    "            article = article.replace(entity+' ','')\n",
    "        elif \"(\"+entity+' ' in article: # added later\n",
    "            article = article.replace(entity+' ','')\n",
    "        elif \"[\"+entity+']' in article:\n",
    "            article = article.replace(entity,'')\n",
    "        elif \"(\"+entity+')' in article: # added later\n",
    "            article = article.replace(entity,'')\n",
    "        elif \"'\"+entity+\"'\" in article:\n",
    "            article = article.replace(entity,'')\n",
    "        elif '\"'+entity+'\"' in article:\n",
    "            article = article.replace(entity,'')\n",
    "    return(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reffinal_df['text_noent'] = [remove_entities(x) for x in reffinal_df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_data_cleaned_named_ent_removed.pickle', 'wb') as file:\n",
    "    pickle.dump(reffinal_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reffinal_df['tokenized_text'] = [word_tokenize(x) for x in reffinal_df['text_noent']]\n",
    "# Remove punctuation\n",
    "reffinal_df['tokenized_nopunc'] = [[word for word in x if word.isalpha()] for x in reffinal_df['tokenized_text']]\n",
    "# Remove capitalization\n",
    "reffinal_df['tokenized_nopunc_lower'] = [[word.lower() for word in x] for x in reffinal_df['tokenized_nopunc']]\n",
    "\n",
    "# Alternative method, if we'd be interested in keeping numbers as well:\n",
    "# import string\n",
    "# exclude = set(string.punctuation) \n",
    "# punc_free = ''.join(ch for ch in stop_free if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "reffinal_df.iloc[200]['tokenized_nopunc_lower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will refine your stopwords.  You may want to import a file as well.\n",
    "custom_stop_words = ['ab', 'al', 'alten', 'america', 'atlanta', 'au', 'av', 'avrov', 'b', 'ba', 'bauer', 'berlin', 'BOOK',\n",
    "                    'boston', 'brill', 'brown', 'c', 'cad', 'cambridge', 'cf', 'ch', 'chap', 'chapter', 'charles',\n",
    "                    'chicago', 'chs', 'cit', 'cite', 'claremont', 'college', 'craig', 'cum', 'd', 'dans', 'de', 'dennis',\n",
    "                    'diese', 'dissertation', 'dm', 'dtr', 'ed', 'eds', 'eerdmans', 'ek', 'elisabeth', 'en', 'et',\n",
    "                    'ev', 'ez', 'f', 'far', 'ff', 'fiir', 'g', 'gar', 'george', 'geschichte', 'gott', 'gottes',\n",
    "                    'grand', 'h', 'ha', 'hall', 'hartford', 'hat', 'haven', 'henry', 'I', 'ia', 'ibid', 'io',\n",
    "                    'isbn', 'iv', 'ivye', 'ix', 'jeremias', 'jesu', 'k', 'ka', 'kai', 'kal', 'kat', 'kee', 'ki', 'kim',\n",
    "                    'kirche', 'klein', 'knox', 'l', 'la', 'le', 'leiden', 'leipzig', 'les', 'life', 'line', 'loc', 'louisville', 'm',\n",
    "                    'ma', 'madison', 'marie', 'marshall', 'mohr', 'n', 'na', 'neuen', 'ni', 'nu', 'nur', 'o', 'ol',\n",
    "                    'om', 'op', 'ov', 'ovadd', 'ovk', 'oxford', 'paper', 'pp', 'paulus', 'ph', 'philadelphia', 'point', 'post',\n",
    "                    'pres', 'president', 'press', 'pro', 'prof', 'professor','quod', 'r', 'ra', 'rab', 'rapids', 'refer', 'review','REVIEWS'\n",
    "                    'reviews', 'ro', 'robert', 'robinson', 'rov', 's', 'sa', 'schmidt', 'schriften', 'scott', 'sec',\n",
    "                    'section', 'seiner', 'sheffield', 'siebeck', 'stanely', 'studien', 't', 'text', 'thee', 'theologie',\n",
    "                    'they', 'thing', 'thou', 'thy', 'tiibingen','tion', 'tov', 'tr', 'tv', 'u', 'um', 'univ', 'University', 'unto', 'v',\n",
    "                    'van', 'verse','view', 'vol', 'volume', 'vs', 'vss', 'vv', 'w', 'william', 'world' 'wunt',\n",
    "                    'y', 'yap', 'ye', 'york', 'zeit','-PRON-', 'jews','jewish', 'judaism', 'page_sequence','page','book','text','doe', \n",
    "                    'books','publish','include','say','die','der','des','das','und','ha','ha-','new','ica','ceede', 'sequence', \n",
    "                     'ibn', 'ben','say','br','ts','aj','thing','iii','nx','va','pr','give','way','nn','im','ny','mn','rn','nm',\n",
    "                    'ri','nl','gt']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk_stop = nlp.Defaults.stop_words\n",
    "nltk_list=list(nltk_stop)\n",
    "en_stop = sorted(list(nltk_list + custom_stop_words))\n",
    "# en_stop=en_stop.extend (custom_stop_words)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reffinal_df['tokenized_nopunc_lower_nostop'] = [[word for word in x if not word in en_stop] for x in reffinal_df['tokenized_nopunc_lower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "len(reffinal_df.iloc[500]['tokenized_nopunc_lower_nostop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just another way to keep adding stopwords to remove from the text\n",
    "extra_stop_words = [\n",
    "    'big','small','low','high',\n",
    "    'none',\n",
    "    'may',\n",
    "    'among',\n",
    "    'within',\n",
    "    'don','t',\n",
    "    'day',\n",
    "    'etc',\n",
    "    'around',\n",
    "    'frequent',\n",
    "    'including',\n",
    "    'even',\n",
    "    'can',\n",
    "    'likely',\n",
    "    'will',\n",
    "    'like',\n",
    "    'today',\n",
    "    'bit',\n",
    "    'put',\n",
    "    'aim',\n",
    "    's',\n",
    "    'got',\n",
    "    'really',\n",
    "    'huge',\n",
    "    'see',\n",
    "    'almost',\n",
    "    'already',\n",
    "    'much',\n",
    "    'recent',   #\n",
    "    'many',\n",
    "    'change',    #\n",
    "    'changes',       #\n",
    "    'someone',\n",
    "    'said',\n",
    "    'says',\n",
    "    'gives',\n",
    "    'give',\n",
    "#     'people',\n",
    "    'new',\n",
    "    'say',\n",
    "    'least','first','last','second',\n",
    "    'one','two',\n",
    "    'go',\n",
    "    'goes',\n",
    "    'take',\n",
    "    'going',\n",
    "    'taking',\n",
    "    'just',\n",
    "    'can'\n",
    "    'cannot',\n",
    "    'keep',\n",
    "    'keeps',\n",
    "    'also',\n",
    "    'done',\n",
    "    'good',\n",
    "    'get',\n",
    "    'without',\n",
    "    'told',\n",
    "    'might',\n",
    "    'time',\n",
    "    'unable',  #\n",
    "    'able',  #\n",
    "    'know',\n",
    "    'end',\n",
    "    'now',\n",
    "    'want',\n",
    "    'didn',\n",
    "    'back',\n",
    "    'doesn',\n",
    "    'couldn',\n",
    "    'since',\n",
    "    'shouldn',\n",
    "    'seen',\n",
    "    'works',\n",
    "    'zero',\n",
    "    'every',\n",
    "    'each',\n",
    "    'other',\n",
    "    'ever',\n",
    "    'neither',\n",
    "    'll',\n",
    "    'mr',\n",
    "    'ms',\n",
    "    'mrs',\n",
    "    'think',\n",
    "    'tomorrow',\n",
    "    'way',\n",
    "    'still',\n",
    "    'know',\n",
    "    'later',\n",
    "    'fine',    #\n",
    "    'let',\n",
    "    'went',\n",
    "    'night',\n",
    "    've',\n",
    "    'must',\n",
    "    'act',  #\n",
    "    're',\n",
    "    'c','b', 'a',\n",
    "    'done',\n",
    "    'began',\n",
    "    'ones',\n",
    "    'm',\n",
    "    'soon',\n",
    "    'word',\n",
    "    'along',\n",
    "    'main',\n",
    "    'q',\n",
    "    'lot',\n",
    "    'e', 'd',\n",
    "    'entire',\n",
    "    'year',\n",
    "    'mean',\n",
    "    'means',\n",
    "    'important',\n",
    "    'always',\n",
    "    'something',\n",
    "    'rather',\n",
    "    'either',\n",
    "    'makes',\n",
    "    'make',\n",
    "    'uses',\n",
    "    'use',\n",
    "    'enough',\n",
    "    'w','d',\n",
    "    'never',\n",
    "    'giving',\n",
    "    'o',\n",
    "    'involve',\n",
    "    'involes',\n",
    "    'involving',\n",
    "    'little',\n",
    "    'inside',\n",
    "    'sat',\n",
    "    'third','fourth','fifth','sixth',\n",
    "    'next',\n",
    "    'given',\n",
    "    'million','billion','millions','billions',\n",
    "    'option',\n",
    "    'options',\n",
    "    'full',\n",
    "    'complete',\n",
    "    'need',\n",
    "    'needs',\n",
    "    'set',\n",
    "    'manage',\n",
    "    'sets',\n",
    "    'manages',\n",
    "    'bring','brings','brought',\n",
    "    'try','tries','tried'\n",
    "    'week',\n",
    "    'former',\n",
    "    'monday','tuesday','wednesday','thursday','friday','saturday','sunday',\n",
    "    'spent','spend', 'spends',\n",
    "    'month','months',\n",
    "    'send','sends','sent',\n",
    "    'went',\n",
    "    'january','february','march','april','may','june','july','august','september','october','november','december',\n",
    "    'allow',\n",
    "    'process',\n",
    "#     'old',\n",
    "    'times',\n",
    "    'nearly',\n",
    "    'looking','looks','look',\n",
    "    'thinly',\n",
    "    'becoming',\n",
    "    'stay','stays',\n",
    "    'took','takes','take',\n",
    "    'types', 'type',\n",
    "    'thought', 'though',\n",
    "    'idea',\n",
    "    'clear','clearly',\n",
    "    'behind',\n",
    "    'half',\n",
    "    'us',\n",
    "    'less',\n",
    "    'claim','claims',\n",
    "    'long', 'short',\n",
    "    'smaller','larger','bigger','largest','biggest','smallest','longer','shorter','short','long',\n",
    "    'extreme','severe',\n",
    "    'largely',\n",
    "    'anymore',\n",
    "    'years',\n",
    "    'spoke',\n",
    "    'give','gave','given','gives',\n",
    "    'reportedly','supposedly','alledgedly',\n",
    "    'please',\n",
    "    'received','receive','receives',\n",
    "    'longtime',\n",
    "    'best',\n",
    "    'existing',\n",
    "    'putting','put','puts',\n",
    "    \n",
    "    'whose',\n",
    "    'yesterday',\n",
    "    \n",
    "    \n",
    "    \n",
    "    'thing',   #added later\n",
    "    'week',\n",
    "    'another',\n",
    "    'month',\n",
    "    'day',\n",
    "    'come']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reffinal_df['tokenized_nopunc_lower_nostop_extra'] = [[word for word in x if not word in extra_stop_words] for x in reffinal_df['tokenized_nopunc_lower_nostop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reffinal_df['tokenized_nopunc_lower_nostop_extra_lemmatized'] = [[lemma.lemmatize(word) for word in x] for x in reffinal_df['tokenized_nopunc_lower_nostop_extra']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_data_full.pickle', 'wb') as file:\n",
    "    pickle.dump(reffinal_df, file)\n",
    "\n",
    "    # Drop interim colums\n",
    "raw_small = pd.DataFrame(reffinal_df['tokenized_nopunc_lower_nostop_extra_lemmatized'])\n",
    "raw_small.rename(columns={'tokenized_nopunc_lower_nostop_extra_lemmatized':'article_text'},inplace=True)\n",
    "\n",
    "with open('clean_data_small.pickle', 'wb') as file:\n",
    "    pickle.dump(raw_small, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_article_interim=pd.DataFrame(reffinal_df[reffinal_df.type.eq('research-article')])\n",
    "book_review_interim=pd.DataFrame(reffinal_df[reffinal_df.type.eq('book-review')])\n",
    "research_article_small = pd.DataFrame(research_article_interim['tokenized_nopunc_lower_nostop_extra_lemmatized'])\n",
    "research_article_small.rename(columns={'tokenized_nopunc_lower_nostop_extra_lemmatized':'article_text'},inplace=True)\n",
    "book_review_small = pd.DataFrame(book_review_interim['tokenized_nopunc_lower_nostop_extra_lemmatized'])\n",
    "book_review_small.rename(columns={'tokenized_nopunc_lower_nostop_extra_lemmatized':'article_text'},inplace=True)\n",
    "citations_small=pd.DataFrame(reffinal_df.filter(['id','citations'],axis=1))\n",
    "\n",
    "with open('clean_data_research_small.pickle', 'wb') as file:\n",
    "    pickle.dump(research_article_small, file)\n",
    "\n",
    "with open('clean_data_book_small.pickle', 'wb') as file:\n",
    "    pickle.dump(book_review_small, file)\n",
    "\n",
    "with open('clean_data_citations_small.pickle', 'wb') as file:\n",
    "    pickle.dump(citations_small, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_small.to_csv('citations.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
