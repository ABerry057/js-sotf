{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can create multiple topic model files according to types and date ranges.  We begin with the same basic dataframe we have been using, clean it up a bit, and then set the parameters and run the routine.  It could take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_data_full.pickle', 'rb') as file:\n",
    "    art = pickle.load(file)\n",
    "# List of Extra Stop words\n",
    "stopwords = [\n",
    "    'week','another','thing','month','day','come',\n",
    "    'york','away','left','wrote','came','tell','asked',\n",
    "    'left','right','hand','point','often','talk','head','point','ago','whether',\n",
    "    'hour','group','became','become','becomes','often','sometimes','usually','page','sequence','doi','p'\n",
    "    ]\n",
    "# Remove Extra Stop Words\n",
    "art['article_text_nostop_extra'] = [[word for word in x if not word in stopwords and len(word)>2] for x in art['tokenized_nopunc_lower_nostop_extra_lemmatized']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art['string'] = [' '.join(x) for x in art['article_text_nostop_extra']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is easy enough to add another atype, 'research-article', and create another loop to run that along with the book-review articles.\n",
    "#We did not do that because we used different numbers of topics for each of these atypes.\n",
    "atype='book-review'\n",
    "years = [ #range (1982,1985),\n",
    "          range(1985,1990),\n",
    "          range(1990,1995),\n",
    "          range(1995,2000), \n",
    "          range(2000,2005), \n",
    "          range(2005,2010),\n",
    "          range(2010,2015)\n",
    "        ]\n",
    "art=art[art['type']==atype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list=[]\n",
    "\n",
    "for i in years:\n",
    "    newart=art.copy()\n",
    "    newart.astype({'year':'int'}).dtypes\n",
    "    x=int(i[0])-1\n",
    "    y=int(i[-1])\n",
    "    newart=newart[newart['year']<y]\n",
    "    newart=newart[newart['year']>x]\n",
    "    texts=newart['article_text_nostop_extra'].tolist()\n",
    "    for a in range(len(texts)):\n",
    "        for b in range(len (texts[a])):\n",
    "            dict_list.append(texts[a][b])\n",
    "    \n",
    "    res = [sub.split() for sub in dict_list] \n",
    "    \n",
    "    dictionary = corpora.Dictionary(res)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "   \n",
    "    corpora.MmCorpus.serialize('corpus_allbooks'+str(i[0])+'.mm', corpus)\n",
    "    no_of_topics = 17\n",
    "    ldamodel17 = LdaModel(corpus, num_topics=no_of_topics, id2word=dictionary, passes=50, alpha='auto', eval_every=2000)\n",
    "    # Save Model (24)\n",
    "    ldamodel17.save('l2a_17t_50p_autoalpha_books_all_{}_val.model'.format(i[0]))\n",
    "    vis = pyLDAvis.gensim.prepare(ldamodel17, corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis,'visualization_research_all'+str(i[0])+'.html')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for a in range(len(texts)):\n",
    "        for b in range(len (texts[a])):\n",
    "            dict_list.append(texts[a][b])\n",
    "\n",
    "\n",
    "    res = [sub.split() for sub in dict_list] \n",
    "    dictionary = corpora.Dictionary(res)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    corpora.MmCorpus.serialize('corpus_book'+str(i[0])+'.mm', corpus)\n",
    "    \n",
    "#     vec = CountVectorizer(max_df =.95,min_df = 10,stop_words='english')\n",
    "#     counts = vec.fit_transform(texts).transpose()\n",
    "#     corpus = matutils.Sparse2Corpus(counts)\n",
    "#     with open('full_set_countvec_corpus.'+str(i[0])+'.pickle','wb') as file:\n",
    "#         pickle.dump(corpus,file)\n",
    "    \n",
    "    no_of_topics = 17\n",
    "    ldamodel17 = LdaModel(corpus, num_topics=no_of_topics, id2word=dictionary, passes=50, alpha='auto', eval_every=2000)\n",
    "    # Save Model (24)\n",
    "    ldamodel17.save('lda_17t_50p_autoalpha_book_{}_val.model'.format(i[0]))\n",
    "    vis = pyLDAvis.gensim.prepare(ldamodel17, corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis,'visualization_book'+str(i[0])+'.html')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
