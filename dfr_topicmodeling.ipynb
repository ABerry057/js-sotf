{
 "cells": [
  {
   "source": [
    "# DfR Topic Modeling Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program creates topics from JSTOR's \"Data for Research\" (DfR). The program takes two inputs. One is a spreadsheet that has the type of articles (research-article or book-review, which will be known as the \"atype\" below), the ids that will be used to access the OCR files, the year of each article, and the language. The second is a simple text file with the custom stop words, each on a separate line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import spacy\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the file structure here.  The \"directory\" variable should be changed to wherever this file lives.\n",
    "file=os.getcwd()\n",
    "\n",
    "file_path = Path(file).resolve()  # get path of this file\n",
    "base_dir = file_path.parents[0]  # get path of parent directory\n",
    "data_dir = base_dir  # get data directory path\n",
    "jstor_dir = data_dir / 'jstor_data'  # get path to jstor_data path\n",
    "corpus_dir = data_dir # get path to where the corpus files will be written\n",
    "model_dir = data_dir # get path to where the model files will be written\n",
    "ocr_files = jstor_dir / 'ocr'\n",
    "\n",
    "corpus_dir_str = str(Path(corpus_dir)) # sets variable for the string version of the corpus path\n",
    "model_dir_str = str(Path(model_dir)) # sets variable for the string version of the model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the csv index file of the articles and prints it out, to enable checking. The name of the file should be changed.\n",
    "reference_df = pd.read_csv(data_dir / \"REFERENCE FILE NAME.csv\")\n",
    "\n",
    "reference_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many journals contain abbreviations that are significant for the topics.  The abbreviations should be custom set, in \n",
    "# a list of tuples.  This particular one expands the abbreviations for biblical books found in the Journal of Biblical Literature.\n",
    "\n",
    "books_abbrs = [('gen', 'genesis'),('exod', 'exodus'),('ex', 'exodus'),('lev', 'leviticus'),('num', 'numbers'),\n",
    "               ('deut', 'deuteronomy'),('josh', 'joshua'),('judg', 'judges'), ('jud', 'judges'),('sam', 'samuel'),('kgs', 'kings'),\n",
    "               ('chr', 'chronicles'),('neh', 'nehemiah'),('esth', 'esther'),('ps', 'psalms'),('pss', 'psalms'),\n",
    "               ('prov', 'proverbs'),('eccl', 'ecclesiastes'),('qoh', 'qoheleth'), ('isa', 'isaiah'),\n",
    "               ('jer', 'jeremiah'),('lam', 'lamentations'),('ezek', 'ezekiel'),('hos', 'hosea'),('obad', 'obediah'),\n",
    "               ('mic', 'micah'),('nah', 'nahum'),('hab', 'habakkuk'),('zeph', 'zephaniah'),('hag', 'haggai'),\n",
    "               ('zech', 'zechariah'),('mal', 'malachi'),('matt', 'matthew'),('mk', 'mark'),('lk', 'luke'),\n",
    "               ('jn', 'john'),('rom', 'romans'),('cor', 'corinthians'),('gal', 'galatians'),('eph', 'ephesians'),\n",
    "               ('phil', 'philippians'),('col', 'colossians'),('thess', 'thessalonians'),('tim','timothy'),\n",
    "               ('phlm', 'philemon'),('heb', 'hebrews'),('jas', 'james'),('pet', 'peter'),('rev', 'revelation'),\n",
    "               ('tob', 'tobit'),('jdt', 'judith'), ('wis', 'wisdom of solomon'),('sir', 'sirach'), ('bar', 'baruch'),\n",
    "               ('macc', 'maccabees'), ('esd', 'esdras'), ('tg', 'targum')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words will be filtered from the results.  These stopwords come from three places: The NLP package; any custom file\n",
    "# that you might have; and anything that you want to put in this list.\n",
    "\n",
    "custom_stop_words = ['ab', 'al', 'alten', 'america', 'atlanta', 'au', 'av', 'avrov', 'b', 'ba', 'bauer', 'berlin', 'BOOK',\n",
    "                    'boston', 'brill', 'brown', 'c', 'cad', 'cambridge', 'cf', 'ch', 'chap', 'chapter', 'charles',\n",
    "                    'chicago', 'chs', 'cit', 'cite', 'claremont', 'college', 'craig', 'cum', 'd', 'dans', 'de', 'dennis',\n",
    "                    'diese', 'dissertation', 'dm', 'dtr', 'ed', 'eds', 'eerdmans', 'ek', 'elisabeth', 'en', 'et',\n",
    "                    'ev', 'ez', 'f', 'far', 'ff', 'fiir', 'g', 'gar', 'george', 'geschichte', 'gott', 'gottes',\n",
    "                    'grand', 'h', 'ha', 'hall', 'hartford', 'hat', 'haven', 'henry', 'I', 'ia', 'ibid', 'io',\n",
    "                    'isbn', 'iv', 'ivye', 'ix', 'jeremias', 'jesu', 'k', 'ka', 'kai', 'kal', 'kat', 'kee', 'ki', 'kim',\n",
    "                    'kirche', 'klein', 'knox', 'l', 'la', 'le', 'leiden', 'leipzig', 'les', 'life', 'line', 'loc', 'louisville', 'm',\n",
    "                    'ma', 'madison', 'marie', 'marshall', 'mohr', 'n', 'na', 'neuen', 'ni', 'nu', 'nur', 'o', 'ol',\n",
    "                    'om', 'op', 'ov', 'ovadd', 'ovk', 'oxford', 'paper', 'pp', 'paulus', 'ph', 'philadelphia', 'point', 'post',\n",
    "                    'pres', 'president', 'press', 'pro', 'prof', 'professor','quod', 'r', 'ra', 'rab', 'rapids', 'refer', 'review','REVIEWS'\n",
    "                    'reviews', 'ro', 'robert', 'robinson', 'rov', 's', 'sa', 'schmidt', 'schriften', 'scott', 'sec',\n",
    "                    'section', 'seiner', 'sheffield', 'siebeck', 'stanely', 'studien', 't', 'text', 'thee', 'theologie',\n",
    "                    'they', 'thing', 'thou', 'thy', 'tiibingen','tion', 'tov', 'tr', 'tv', 'u', 'um', 'univ', 'University', 'unto', 'v',\n",
    "                    'van', 'verse','view', 'vol', 'volume', 'vs', 'vss', 'vv', 'w', 'william', 'world' 'wunt',\n",
    "                    'y', 'yap', 'ye', 'york', 'zeit','-PRON-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads all the stop words together, and prints out their number just to check.\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "stop_words.update(custom_stop_words)\n",
    "stop_words.update('custom_stopwords.txt')\n",
    "stop_words=set(stop_words)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of ids from the reference dataframe that meet the criteria for type (book-review)\n",
    "# or article), language, and the year range. These ids are then used to access the ocr files\n",
    "\n",
    "def id_list(atype, years, ltype):\n",
    "    atype_filter = reference_df['type'] == atype\n",
    "    ltype_filter = reference_df['lang'] == ltype\n",
    "    atype_ids = reference_df[atype_filter]\n",
    "    atype_ids = reference_df[ltype_filter]\n",
    "\n",
    "    a_ids = []\n",
    "    file_id_lst_final=[]\n",
    "\n",
    "    for year_lst in years:\n",
    "        for year in year_lst:\n",
    "            ids = atype_ids.loc[atype_ids['year'] == year]['id'].tolist()\n",
    "            a_ids.append(ids)\n",
    "            file_id_lst=[item for sublist in a_ids for item in sublist]\n",
    "            file_id_lst=[item+'.txt' for item in file_id_lst]\n",
    "        file_id_lst_final.append(file_id_lst) \n",
    "    \n",
    "    return file_id_lst_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions process the ocr texts with the NLP tools\n",
    "\n",
    "def substitute(list_tuples, string):\n",
    "    for tuple_ in list_tuples:\n",
    "        string = re.sub(r'\\b' + tuple_[0] + r'\\b', tuple_[1], string)\n",
    "    return string\n",
    "\n",
    "def get_lemmas(doc):\n",
    "    tokens = [token for token in doc]\n",
    "    lemmas = [token.lemma_ for token in tokens if token.is_alpha]\n",
    "    lemmas = [lemma for lemma in lemmas if lemma not in stop_words]\n",
    "    for index, item in enumerate(lemmas):\n",
    "        item = substitute(books_abbrs, item)\n",
    "        lemmas[index] = item\n",
    "    return lemmas\n",
    "\n",
    "def get_noun_lemmas(doc):\n",
    "    tokens = [token for token in doc]\n",
    "    noun_tokens = [token for token in tokens if token.tag_ == 'NN' or token.tag_ == 'NNP' or token.tag_ == 'NNS']\n",
    "    noun_lemmas = [noun_token.lemma_ for noun_token in noun_tokens if noun_token.is_alpha]\n",
    "    noun_lemmas = [noun_lemma for noun_lemma in noun_lemmas if noun_lemma not in stop_words]\n",
    "    for index, item in enumerate(noun_lemmas):\n",
    "        item = substitute(books_abbrs, item)\n",
    "        noun_lemmas[index] = item\n",
    "    return noun_lemmas\n",
    "    \n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = get_lemmas(doc)\n",
    "    noun_lemmas = get_noun_lemmas(doc)\n",
    "    return lemmas, noun_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates unique ids for storing the output files\n",
    "def string_id(dates):\n",
    "    dates = tuple(dates)\n",
    "    years_str = str(dates[0]) + '-' + str(dates[-1])\n",
    "    return years_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the primary function that creates the dictionary and corpus files and writes out the model files.  \n",
    "\n",
    "def prep_models(year_range_lst, years):\n",
    "    \n",
    "    # Takes in a list of ids to ocr files (the year_range_lst), accesses the ocr file, processes the language, and creates the\n",
    "    # general list of all words (general_docs) and the list of nouns.\n",
    "    for item in year_range_lst:\n",
    "        general_docs = []\n",
    "        noun_docs = [] \n",
    "        with open (ocr_files / item, mode='r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            lemmas, nouns = process_text(text)\n",
    "            general_docs.append(lemmas)\n",
    "            noun_docs.append(nouns)\n",
    "        with open (corpus_dir / 'general_docs', encoding='utf8', mode='w') as outfile:\n",
    "            json.dump(general_docs, outfile)\n",
    "        with open (corpus_dir / 'noun_docs', encoding='utf8', mode='w') as outfile:\n",
    "            json.dump(noun_docs, outfile)\n",
    "        \n",
    "# Puts the list into the tokenized form that the dictionary and corpus producing functions below need.    \n",
    "    f = open(corpus_dir / 'general_docs', 'r')\n",
    "    contents = f.read()\n",
    "    f.close()\n",
    "    contents = contents.split(',')\n",
    "    contents = [d.split() for d in contents]\n",
    "        \n",
    "        \n",
    "# create general dictionary\n",
    "    general_dictionary = corpora.Dictionary(contents)\n",
    "    # general_dictionary.filter_extremes(no_below=2, no_above=0.7)\n",
    "    general_dictionary.save(corpus_dir_str + '/general_corpus' + '_' + years +'.dict' )\n",
    "\n",
    "# create general corpus\n",
    "    corpus = [general_dictionary.doc2bow(doc) for doc in contents]\n",
    "    corpora.MmCorpus.serialize(corpus_dir_str + '/general_corpus'+ '_' + years + '.mm', corpus)\n",
    "    \n",
    "    nf = open(corpus_dir / 'noun_docs','r')\n",
    "    ncontents = nf.read()\n",
    "    nf.close()\n",
    "    ncontents = ncontents.split(',')\n",
    "    ncontents = [d.split() for d in ncontents]\n",
    "     \n",
    "        \n",
    "# create noun dictionary\n",
    "    noun_dictionary = corpora.Dictionary(ncontents)\n",
    "    # noun_dictionary.filter_extremes(no_below=10, no_above=0.7)\n",
    "    noun_dictionary.save(corpus_dir_str + '/noun_corpus' + '_' + years + '.dict')\n",
    "    \n",
    "\n",
    "# create noun corpus\n",
    "    ncorpus = [noun_dictionary.doc2bow(doc) for doc in ncontents]\n",
    "    corpora.MmCorpus.serialize(corpus_dir_str + '/noun_corpus'+ '_' + years + '.mm', corpus)\n",
    "    \n",
    "    \n",
    "# run topic models and save.  These can be changed to different numbers of topics\n",
    "    nlda_25 = models.LdaModel(ncorpus, id2word=noun_dictionary, num_topics=25, passes=100, random_state=42)\n",
    "    nlda_25.save(model_dir_str + '/noun_25_' + years + '.model')\n",
    "    \n",
    "    glda_25 = models.LdaModel(corpus, id2word=general_dictionary, num_topics=25, passes=100, random_state=42)\n",
    "    glda_25.save(model_dir_str + '/general_25_' + years + '.model')\n",
    "        \n",
    "    return nlda_25, ncorpus, noun_dictionary, noun_docs, glda_25, corpus, general_dictionary, general_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The files are output in the '...data/corpus' directory.  It is a good idea to make sure that before you run this step you make\n",
    "# sure that the directory exists and is empty (not strictly necessary, but better for the next step).\n",
    "\n",
    "\n",
    "\n",
    "# These next three statements should be modified for the query.  atype can be 'book-review' or 'research-article'.  years is a list of range\n",
    "# objects, with the beginning and ending years (N.B.: results will not include the last year) separated by a comma, in \n",
    "# parentheses. Language will usually be \"eng\" for English\n",
    "atype = 'research-article'\n",
    "years = [range(1984, 1985), \n",
    "        # range(1992,1995)\n",
    "        # range(1991,1996),\n",
    "        # range(1996,2001), \n",
    "        # range(2001,2006), \n",
    "        # range(2006,2011),\n",
    "        # range(2011,2015)\n",
    "        ]\n",
    "ltype = 'eng'\n",
    "\n",
    "\n",
    "# Calls the function to generate the lists of ids.  This comes back as a list of lists (one for each date range), and within \n",
    "# each one are the file ids needed to access the relevant ocr files\n",
    "final_lst_id = []\n",
    "final_lst_id = id_list(atype, years, ltype)\n",
    "\n",
    "# Passes each list in final_lst_id to a function that (1) creates and cleans the files needed for the topic modeling; \n",
    "# and (2) runs a topic model; and (3) saves the results of the model in a unique file.\n",
    "i = 0\n",
    "for year_range_lst in final_lst_id:\n",
    "    yr_range = string_id(years[i])\n",
    "    noun_model, ncorpus, noun_dictionary, noun_docs, general_model, corpus, general_dictionary, general_docs = prep_models(year_range_lst, yr_range)\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A general command to print out the topics\n",
    "\n",
    "pprint(general_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_compositions = {}\n",
    "for i in range(len(corpus)):\n",
    "    doc_compositions[i] = general_model.get_document_topics(corpus[i])\n",
    "# doc_compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_dictionary.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in year_range_lst:\n",
    "        with open (ocr_files / item, mode='r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            break\n",
    "\n",
    "# general_dictionary.doc2bow(text.split(' ')[0])\n",
    "general_dictionary.doc2idx(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('ajs_review': conda)",
   "language": "python",
   "name": "python38264bitajsreviewconda38786f0edd414401891a87f582d93747"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}