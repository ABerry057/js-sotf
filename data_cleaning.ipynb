{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitajsreviewconda38786f0edd414401891a87f582d93747",
   "display_name": "Python 3.8.2 64-bit ('ajs_review': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Cleaning Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Import the necessary modules and download the stopword list from the NLTK source. Define a stopword list by limiting the downloaded stopwords to English words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "source": [
    "Choose an article type to create sparklines for. Possible choices are 'research-article' and 'book-review'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_type = 'research-article'"
   ]
  },
  {
   "source": [
    "Declare a variable to hold the path to the notebook and a variable to hold the path to the unigram data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = Path(os.getcwd()).resolve()\n",
    "unigram_dir = file_dir / 'ngram1'"
   ]
  },
  {
   "source": [
    "The following code block defines a function which collects all the unigram data for the given article type and an optionally-given list of article IDs. It returns a dataframe with the unigrams as one column and their corresponding counts as the second column. The dataframe is sorted by unigram count in descending order."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unigram_table(atype, a_ids=None):\n",
    "    \"\"\"Makes a dataframe by concatenating all unigrams from\n",
    "        articles specified by a_ids.\n",
    "\n",
    "    Arguments:\n",
    "        atype {String} -- the article type to source unigrams\n",
    "                          from: 'book-review', 'research-article',\n",
    "                          or 'both'.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        a_id {List} -- List of article IDs to include, by default\n",
    "                       all articles are included. (default: {None})\n",
    "    \"\"\"\n",
    "    if atype == \"both\":\n",
    "        raise ValueError(\"Feature not yet supported!\\n\")\n",
    "    curr_path = unigram_dir / atype\n",
    "    dfs = []\n",
    "    for f in tqdm(curr_path.iterdir()):\n",
    "        f_id = re.search(f\"{atype}/(.*)-n\", str(f)).group(1)\n",
    "        if a_ids is not None:\n",
    "            if f_id in a_ids:\n",
    "                # print(\"Match!\")  # for debugging\n",
    "                unigrams = pd.read_csv(f, sep='\\t', names=[\"word\", \"count\"])\n",
    "                # print(f\"Just read {f_id}\")  # for debugging\n",
    "                dfs.append(unigrams)\n",
    "            else:\n",
    "                # print(f\"{f_id} not in a_ids list\")\n",
    "                continue\n",
    "        else:\n",
    "            unigrams = pd.read_csv(f, sep='\\t', names=[\"word\", \"count\"])\n",
    "            # print(f\"Just read {f_id}\")  # for debugging\n",
    "            dfs.append(unigrams)\n",
    "    print(f\"\\nCollected {len(dfs)} articles\\n\")\n",
    "    if not dfs:\n",
    "        raise ValueError(\"Unable to collect dataframes\")\n",
    "    concat_df = pd.concat(dfs)\n",
    "    summed_df = concat_df.groupby([\"word\"]).sum().reset_index()\n",
    "    sorted_df = summed_df.sort_values(by=[\"count\"], ascending=False)\n",
    "    return sorted_df"
   ]
  },
  {
   "source": [
    "Call the function with the given article type and inspect the resulting dataframe of unigrams and their counts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "392it [00:01, 372.74it/s]\n\nCollected 392 articles\n\nThe unigram dataframe has size: (209491, 2)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       word  count\n153833    s  35062\n3270      1  28182\n76686   his  26923\n114170    n  23915\n78330     i  22514",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>153833</th>\n      <td>s</td>\n      <td>35062</td>\n    </tr>\n    <tr>\n      <th>3270</th>\n      <td>1</td>\n      <td>28182</td>\n    </tr>\n    <tr>\n      <th>76686</th>\n      <td>his</td>\n      <td>26923</td>\n    </tr>\n    <tr>\n      <th>114170</th>\n      <td>n</td>\n      <td>23915</td>\n    </tr>\n    <tr>\n      <th>78330</th>\n      <td>i</td>\n      <td>22514</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "unigrams = make_unigram_table(article_type)\n",
    "print(f\"The unigram dataframe has size: {unigrams.shape}\")\n",
    "unigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing any unigrams with counts below a predetermined threshold can result in quicker processing and more succint analysis. The following function does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_counts(df, threshold):\n",
    "    \"\"\"Removes rows from an ngram table with counts fewer than threshold.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas dataframe} -- ngram table with columns \"word\" and \"count\"\n",
    "        threshold {Integer} -- minimum count to keep rows\n",
    "    \"\"\"\n",
    "    dropped_df = df[df['count'] >= threshold]\n",
    "    return dropped_df"
   ]
  },
  {
   "source": [
    "After filtering unigrams by their counts with a threshold of 10, the number of unigrams decreases from ~200,000 to -25,000."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The truncated unigrams dataframe has size: (24863, 2)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       word  count\n153833    s  35062\n3270      1  28182\n76686   his  26923\n114170    n  23915\n78330     i  22514",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>153833</th>\n      <td>s</td>\n      <td>35062</td>\n    </tr>\n    <tr>\n      <th>3270</th>\n      <td>1</td>\n      <td>28182</td>\n    </tr>\n    <tr>\n      <th>76686</th>\n      <td>his</td>\n      <td>26923</td>\n    </tr>\n    <tr>\n      <th>114170</th>\n      <td>n</td>\n      <td>23915</td>\n    </tr>\n    <tr>\n      <th>78330</th>\n      <td>i</td>\n      <td>22514</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "truncated_unigrams = drop_counts(unigrams, 10)\n",
    "print(f\"The truncated unigrams dataframe has size: {truncated_unigrams.shape}\")\n",
    "truncated_unigrams.head()"
   ]
  },
  {
   "source": [
    "The following function lemmatizes the unigrams in the table. Lemmatization is the process of aiming to remove inflectional endings of a word and to return the base or dictionary form, which is known as the lemma. For example, \"car\", \"cars\", \"car's\", and \"cars'\" all map to the base for of 'car'. This reduces noise and allows for more accurate analysis of any underlying patterns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_table(df):\n",
    "    \"\"\"Lemmatizes the words in the word column of an ngram table. Sums\n",
    "       frequencies of words that map to the same lemma.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas dataframe} -- ngram table with columns 'word' and 'count'.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"The passed object is of None type\")\n",
    "        return \"Error\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_word(word):\n",
    "        \"\"\"Uses lemmatization from spacy to map words to lemmas.\n",
    "           (Could do with some tweaking to better group lemmas).\n",
    "\n",
    "        Arguments:\n",
    "            word {String} -- lowercase word to lemmatize\n",
    "\n",
    "        Returns:\n",
    "            String -- lemmatized word\n",
    "        \"\"\"\n",
    "        return lemmatizer.lemmatize(word)\n",
    "    df_lemma = df.copy()\n",
    "    df_lemma['word'] = df['word'].map(lemmatize_word, na_action=\"ignore\")\n",
    "    summed_df = df_lemma.groupby([\"word\"]).sum().reset_index()\n",
    "    sorted_df = summed_df.sort_values(by=[\"count\"], ascending=False)\n",
    "    return sorted_df"
   ]
  },
  {
   "source": [
    "Call the function on the dataframe of truncated unigrams."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The lemmatized unigram dataframe has size: (22739, 2)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      word  count\n17225    s  35250\n85       1  28275\n9516   his  26923\n13307    n  24388\n9750     i  22514",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17225</th>\n      <td>s</td>\n      <td>35250</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>1</td>\n      <td>28275</td>\n    </tr>\n    <tr>\n      <th>9516</th>\n      <td>his</td>\n      <td>26923</td>\n    </tr>\n    <tr>\n      <th>13307</th>\n      <td>n</td>\n      <td>24388</td>\n    </tr>\n    <tr>\n      <th>9750</th>\n      <td>i</td>\n      <td>22514</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "lemmatized_unigrams = lemmatize_table(truncated_unigrams)\n",
    "print(f\"The lemmatized unigram dataframe has size: {lemmatized_unigrams.shape}\")\n",
    "lemmatized_unigrams.head()"
   ]
  },
  {
   "source": [
    "The following block defines two functions. The first, **is_year** is a helper function that makes an educated guess as to whether a string composed entirely of numbers represents a year or not. It is valid for years between 1000 and 2999 CE.\n",
    "\n",
    "The second function, **remove_numerals** uses **is_year* and removes unigrams from a unigram table that are composed partially or entirely of numerals (depending on a keyword argument which is true by default). The function attemps to retain numerals that most likely represent years, as these are meaningful in analytic contexts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_year(string):\n",
    "    \"\"\"Returns a boolean value if the input string likely represents a year.\n",
    "       Only valid for years 1000-2999.\n",
    "    Arguments:\n",
    "        string {String} -- candidate string\n",
    "    \"\"\"\n",
    "    return bool(re.match(\"(1|2)\\d{3}\", string))\n",
    "\n",
    "\n",
    "def remove_numerals(df, remove_mixed_strings=True):\n",
    "    \"\"\"Removes rows from an ngram table with words that are numerals. This\n",
    "       does not include 4-digit numbers which are interpreted as years.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas dataframe} -- A dataframe of with columns 'word', 'count'.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        remove_mixed_strings {bool} -- Whether to remove rows with words that\n",
    "        are mixtures of numerals and letters. (default: {True})\n",
    "    \"\"\"\n",
    "    no_numerals_df = df.copy().reset_index()\n",
    "    for i, row in tqdm(no_numerals_df.iterrows()):\n",
    "        word = row['word']\n",
    "        if remove_mixed_strings:\n",
    "            if any([c.isnumeric() for c in word]) and \\\n",
    "               not is_year(word):\n",
    "                no_numerals_df.drop(i, axis=0, inplace=True)\n",
    "        else:\n",
    "            if word.isnumeric() and len(word) != 4:\n",
    "                no_numerals_df.drop(i, axis=0, inplace=True)\n",
    "    return no_numerals_df"
   ]
  },
  {
   "source": [
    "Calling the function on the lemmatized unigrams returns the following dataframe:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "22739it [00:07, 3122.87it/s]\nThe unigram dataframe with numerals removed has size: (20992, 3)\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   index word  count\n0  17225    s  35250\n2   9516  his  26923\n3  13307    n  24388\n4   9750    i  22514\n5   9286   he  20488",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17225</td>\n      <td>s</td>\n      <td>35250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9516</td>\n      <td>his</td>\n      <td>26923</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13307</td>\n      <td>n</td>\n      <td>24388</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9750</td>\n      <td>i</td>\n      <td>22514</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9286</td>\n      <td>he</td>\n      <td>20488</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "no_nums_df = remove_numerals(lemmatized_unigrams)\n",
    "print(f\"\\nThe unigram dataframe with numerals removed has size: {no_nums_df.shape}\")\n",
    "no_nums_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If nedded - use the following function to create a custom stop list for eventual removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_custom_stopword_list(df):\n",
    "    \"\"\"Creates and saves a custom stopword list as a plain text file in the data directory.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas Dataframe} -- A dataframe of with columns 'word', 'count', 'stopword'\n",
    "        which is a table of ngram frequencies that has been manually labeled with an 'x'\n",
    "        to indicate inclusion. Words marked with 'r' have already been reviewed and will\n",
    "        not be added to the stop list. All other words are added to the stop list.\n",
    "    \"\"\"\n",
    "    stopwords = []\n",
    "    for i, row in tqdm(df.iterrows(), desc=\"Creating stopwords\\n\"):\n",
    "        stopword_status = row['stopword']\n",
    "        if stopword_status == 'r': # if 'r', word has already been reviewed, so ignore\n",
    "            continue\n",
    "        if stopword_status == 'x': # if 'x', word is marked for inclusion, so ignore\n",
    "            df.loc[i,'stopword'] = 'r' # update 'x' to 'r'\n",
    "        if stopword_status == \"unchecked\": # if 'unchecked', wait for manual review\n",
    "            continue\n",
    "        else:\n",
    "            stopwords.append(row['word'])\n",
    "    with open(\"custom_stopwords.txt\", \"w+\") as filehandle:\n",
    "        for word in stopwords:\n",
    "            filehandle.write(f\"{word}\\n\")"
   ]
  },
  {
   "source": [
    "The following function removes stopwords from a unigram table: either those in the NLTK stopword list or those supplied from a custom list. Crafting a custom stopword list is time-consuming but often worthwhile; the NLTK list is not adapted for the specfic vocabulary of any one field."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, include_custom=False):\n",
    "    \"\"\"Removes rows from an ngram table with words\n",
    "       in a custom stopword list or in NLTK list.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas dataframe} -- ngram table with columns \"word\" and \"stopword\"\n",
    "    \"\"\"\n",
    "    custom_stop = set()\n",
    "    if include_custom:\n",
    "        with open(\"custom_stopwords.txt\") as f:\n",
    "            custom_stop = f.readlines()\n",
    "    custom_stop = [w.strip() for w in custom_stop]\n",
    "    no_stops_df = df.copy()\n",
    "    for i, row in tqdm(no_stops_df.iterrows()):\n",
    "        word = row['word']\n",
    "        if include_custom:\n",
    "            if word in nltk_stopwords or word in custom_stop:\n",
    "                no_stops_df.drop(i, axis=0, inplace=True)\n",
    "        else:\n",
    "            if word in nltk_stopwords:\n",
    "                no_stops_df.drop(i, axis=0, inplace=True)\n",
    "    return no_stops_df"
   ]
  },
  {
   "source": [
    "Calling the function on the unigrams with numerals removed returns the following dataframe:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "20992it [00:02, 7435.10it/s]The unigram dataframe with stopwords removed has size: (20874, 3)\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    index    word  count\n3   13307       n  24388\n7   10909  jewish  17390\n9    8993      ha  15356\n10  17571     see  15304\n11  14554       p  13117",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>13307</td>\n      <td>n</td>\n      <td>24388</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10909</td>\n      <td>jewish</td>\n      <td>17390</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8993</td>\n      <td>ha</td>\n      <td>15356</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>17571</td>\n      <td>see</td>\n      <td>15304</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>14554</td>\n      <td>p</td>\n      <td>13117</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "no_stops_df = remove_stopwords(no_nums_df)\n",
    "print(f\"The unigram dataframe with stopwords removed has size: {no_stops_df.shape}\")\n",
    "no_stops_df.head()"
   ]
  },
  {
   "source": [
    "This is the end of the data cleaning process. Save the final dataframe to a CSV file:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops_df.to_csv(\"articles_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}