{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitajsreviewconda38786f0edd414401891a87f582d93747",
   "display_name": "Python 3.8.2 64-bit ('ajs_review': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_type = 'research-article'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = Path(os.getcwd()).resolve()\n",
    "unigram_dir = file_dir / 'ngram1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unigram_table(atype, a_ids=None):\n",
    "    \"\"\"Makes a dataframe by concatenating all unigrams from\n",
    "        articles specified by a_ids.\n",
    "\n",
    "    Arguments:\n",
    "        atype {String} -- the article type to source unigrams\n",
    "                          from: 'book-review', 'research-article',\n",
    "                          or 'both'.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        a_id {List} -- List of article IDs to include, by default\n",
    "                       all articles are included. (default: {None})\n",
    "    \"\"\"\n",
    "    if atype == \"both\":\n",
    "        raise ValueError(\"Feature not yet supported!\\n\")\n",
    "    curr_path = unigram_dir / atype\n",
    "    dfs = []\n",
    "    for f in tqdm(curr_path.iterdir()):\n",
    "        f_id = re.search(f\"{atype}/(.*)-n\", str(f)).group(1)\n",
    "        if a_ids is not None:\n",
    "            if f_id in a_ids:\n",
    "                # print(\"Match!\")  # for debugging\n",
    "                unigrams = pd.read_csv(f, sep='\\t', names=[\"word\", \"count\"])\n",
    "                # print(f\"Just read {f_id}\")  # for debugging\n",
    "                dfs.append(unigrams)\n",
    "            else:\n",
    "                # print(f\"{f_id} not in a_ids list\")\n",
    "                continue\n",
    "        else:\n",
    "            unigrams = pd.read_csv(f, sep='\\t', names=[\"word\", \"count\"])\n",
    "            # print(f\"Just read {f_id}\")  # for debugging\n",
    "            dfs.append(unigrams)\n",
    "    print(f\"\\nCollected {len(dfs)} articles\\n\")\n",
    "    if not dfs:\n",
    "        raise ValueError(\"Unable to collect dataframes\")\n",
    "    concat_df = pd.concat(dfs)\n",
    "    summed_df = concat_df.groupby([\"word\"]).sum().reset_index()\n",
    "    sorted_df = summed_df.sort_values(by=[\"count\"], ascending=False)\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "392it [00:02, 162.15it/s]\n\nCollected 392 articles\n\nThe unigram dataframe has size: (209491, 2)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       word  count\n153833    s  35062\n3270      1  28182\n76686   his  26923\n114170    n  23915\n78330     i  22514",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>153833</th>\n      <td>s</td>\n      <td>35062</td>\n    </tr>\n    <tr>\n      <th>3270</th>\n      <td>1</td>\n      <td>28182</td>\n    </tr>\n    <tr>\n      <th>76686</th>\n      <td>his</td>\n      <td>26923</td>\n    </tr>\n    <tr>\n      <th>114170</th>\n      <td>n</td>\n      <td>23915</td>\n    </tr>\n    <tr>\n      <th>78330</th>\n      <td>i</td>\n      <td>22514</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "unigrams = make_unigram_table(article_type)\n",
    "print(f\"The unigram dataframe has size: {unigrams.shape}\")\n",
    "unigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing any unigrams with counts below a predetermined threshold can result in quicker processing and more succint analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_counts(df, threshold):\n",
    "    \"\"\"Removes rows from an ngram table with counts fewer than threshold.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas dataframe} -- ngram table with columns \"word\" and \"count\"\n",
    "        threshold {Integer} -- minimum count to keep rows\n",
    "    \"\"\"\n",
    "    dropped_df = df[df['count'] >= threshold]\n",
    "    return dropped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The truncated unigrams dataframe has size: (24863, 2)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       word  count\n153833    s  35062\n3270      1  28182\n76686   his  26923\n114170    n  23915\n78330     i  22514",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>153833</th>\n      <td>s</td>\n      <td>35062</td>\n    </tr>\n    <tr>\n      <th>3270</th>\n      <td>1</td>\n      <td>28182</td>\n    </tr>\n    <tr>\n      <th>76686</th>\n      <td>his</td>\n      <td>26923</td>\n    </tr>\n    <tr>\n      <th>114170</th>\n      <td>n</td>\n      <td>23915</td>\n    </tr>\n    <tr>\n      <th>78330</th>\n      <td>i</td>\n      <td>22514</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "truncated_unigrams = drop_counts(unigrams, 10)\n",
    "print(f\"The truncated unigrams dataframe has size: {truncated_unigrams.shape}\")\n",
    "truncated_unigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_table(df):\n",
    "    \"\"\"Lemmatizes the words in the word column of an ngram table. Sums\n",
    "       frequencies of words that map to the same lemma.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas dataframe} -- ngram table with columns 'word' and 'count'.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"The passed object is of None type\")\n",
    "        return \"Error\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_word(word):\n",
    "        \"\"\"Uses lemmatization from spacy to map words to lemmas.\n",
    "           (Could do with some tweaking to better group lemmas).\n",
    "\n",
    "        Arguments:\n",
    "            word {String} -- lowercase word to lemmatize\n",
    "\n",
    "        Returns:\n",
    "            String -- lemmatized word\n",
    "        \"\"\"\n",
    "        return lemmatizer.lemmatize(word)\n",
    "    df_lemma = df.copy()\n",
    "    df_lemma['word'] = df['word'].map(lemmatize_word, na_action=\"ignore\")\n",
    "    summed_df = df_lemma.groupby([\"word\"]).sum().reset_index()\n",
    "    sorted_df = summed_df.sort_values(by=[\"count\"], ascending=False)\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The lemmatized unigram dataframe has size: (22739, 2)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      word  count\n17225    s  35250\n85       1  28275\n9516   his  26923\n13307    n  24388\n9750     i  22514",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17225</th>\n      <td>s</td>\n      <td>35250</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>1</td>\n      <td>28275</td>\n    </tr>\n    <tr>\n      <th>9516</th>\n      <td>his</td>\n      <td>26923</td>\n    </tr>\n    <tr>\n      <th>13307</th>\n      <td>n</td>\n      <td>24388</td>\n    </tr>\n    <tr>\n      <th>9750</th>\n      <td>i</td>\n      <td>22514</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "lemmatized_unigrams = lemmatize_table(truncated_unigrams)\n",
    "print(f\"The lemmatized unigram dataframe has size: {lemmatized_unigrams.shape}\")\n",
    "lemmatized_unigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following function to include custom stop words into the stop word list for eventual removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_custom_stopword_list(df):\n",
    "    \"\"\"Creates and saves a custom stopword list as a plain text file in the data directory.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas Dataframe} -- A dataframe of with columns 'word', 'count', 'stopword'\n",
    "        which is a table of ngram frequencies that has been manually labeled with an 'x'\n",
    "        to indicate inclusion. Words marked with 'r' have already been reviewed and will\n",
    "        not be added to the stop list. All other words are added to the stop list.\n",
    "    \"\"\"\n",
    "    stopwords = []\n",
    "    for i, row in tqdm(df.iterrows(), desc=\"Creating stopwords\\n\"):\n",
    "        stopword_status = row['stopword']\n",
    "        if stopword_status == 'r': # if 'r', word has already been reviewed, so ignore\n",
    "            continue\n",
    "        if stopword_status == 'x': # if 'x', word is marked for inclusion, so ignore\n",
    "            df.loc[i,'stopword'] = 'r' # update 'x' to 'r'\n",
    "        if stopword_status == \"unchecked\": # if 'unchecked', wait for manual review\n",
    "            continue\n",
    "        else:\n",
    "            stopwords.append(row['word'])\n",
    "    with open(data_dir / \"custom_stopwords.txt\", \"w+\") as filehandle:\n",
    "        for word in stopwords:\n",
    "            filehandle.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_year(string):\n",
    "    \"\"\"Returns a boolean value if the input string likely represents a year.\n",
    "       Only valid for years 1000-2999.\n",
    "    Arguments:\n",
    "        string {String} -- candidate string\n",
    "    \"\"\"\n",
    "    return bool(re.match(\"(1|2)\\d{3}\", string))\n",
    "\n",
    "\n",
    "def remove_numerals(df, remove_mixed_strings=True):\n",
    "    \"\"\"Removes rows from an ngram table with words that are numerals. This\n",
    "       does not include 4-digit numbers which are interpreted as years.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas dataframe} -- A dataframe of with columns 'word', 'count'.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        remove_mixed_strings {bool} -- Whether to remove rows with words that\n",
    "        are mixtures of numerals and letters. (default: {True})\n",
    "    \"\"\"\n",
    "    no_numerals_df = df.copy().reset_index()\n",
    "    for i, row in tqdm(no_numerals_df.iterrows()):\n",
    "        word = row['word']\n",
    "        if remove_mixed_strings:\n",
    "            if any([c.isnumeric() for c in word]) and \\\n",
    "               not is_year(word):\n",
    "                no_numerals_df.drop(i, axis=0, inplace=True)\n",
    "        else:\n",
    "            if word.isnumeric() and len(word) != 4:\n",
    "                no_numerals_df.drop(i, axis=0, inplace=True)\n",
    "    return no_numerals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "22739it [00:11, 1964.61it/s]The unigram dataframe with numerals removed has size: (20992, 3)\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   index word  count\n0  17225    s  35250\n2   9516  his  26923\n3  13307    n  24388\n4   9750    i  22514\n5   9286   he  20488",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17225</td>\n      <td>s</td>\n      <td>35250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9516</td>\n      <td>his</td>\n      <td>26923</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13307</td>\n      <td>n</td>\n      <td>24388</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9750</td>\n      <td>i</td>\n      <td>22514</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9286</td>\n      <td>he</td>\n      <td>20488</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "no_nums_df = remove_numerals(lemmatized_unigrams)\n",
    "print(f\"The unigram dataframe with numerals removed has size: {no_nums_df.shape}\")\n",
    "no_nums_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, include_custom=False):\n",
    "    \"\"\"Removes rows from an ngram table with words\n",
    "       in a custom stopword list or in NLTK list.\n",
    "\n",
    "    Arguments:\n",
    "        df {Pandas dataframe} -- ngram table with columns \"word\" and \"stopword\"\n",
    "    \"\"\"\n",
    "    custom_stop = set()\n",
    "    if include_custom:\n",
    "        with open(data_dir / \"custom_stopwords.txt\") as f:\n",
    "            custom_stop = f.readlines()\n",
    "    custom_stop = [w.strip() for w in custom_stop]\n",
    "    no_stops_df = df.copy()\n",
    "    for i, row in tqdm(no_stops_df.iterrows()):\n",
    "        word = row['word']\n",
    "        if include_custom:\n",
    "            if word in nltk_stopwords or word in custom_stop:\n",
    "                no_stops_df.drop(i, axis=0, inplace=True)\n",
    "        else:\n",
    "            if word in nltk_stopwords:\n",
    "                no_stops_df.drop(i, axis=0, inplace=True)\n",
    "    return no_stops_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "20992it [00:04, 4334.15it/s]The unigram dataframe with stopwords removed has size: (20874, 3)\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    index    word  count\n3   13307       n  24388\n7   10909  jewish  17390\n9    8993      ha  15356\n10  17571     see  15304\n11  14554       p  13117",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>13307</td>\n      <td>n</td>\n      <td>24388</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10909</td>\n      <td>jewish</td>\n      <td>17390</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8993</td>\n      <td>ha</td>\n      <td>15356</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>17571</td>\n      <td>see</td>\n      <td>15304</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>14554</td>\n      <td>p</td>\n      <td>13117</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "no_stops_df = remove_stopwords(no_nums_df)\n",
    "print(f\"The unigram dataframe with stopwords removed has size: {no_stops_df.shape}\")\n",
    "no_stops_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}