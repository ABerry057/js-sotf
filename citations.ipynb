{
 "cells": [
  {
   "source": [
    "# Citations Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help to use JSTOR Data for Research (DfR) of journal runs and create citation networks from them. It takes in a CSV file produced from the DfR metadata. The important columns for this program are the \"article_author\" and the \"citation_general\", the latter being the entire text of each \"mixed-citation\" field in the DfR metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('YOUR CITATIONS CSV FILE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compiled a list of the authors of the research articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_authors = df['article_author'].tolist()\n",
    "art_authors_set = set(art_authors)\n",
    "art_authors = list(art_authors_set)\n",
    "print(art_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that there were some strange characters in our list, particularly due to unicode characters.  We used the following code to clean these.  Your own strange characters may, of course, differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for elem in art_authors:\n",
    "    elem = str(elem)\n",
    "    elem = elem.replace('\\u202e','')\n",
    "    elem = elem.replace('\\u202c','')\n",
    "    x.append(elem)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next extracted from the citation_general column the likely names of authors.  To do this, pattern-matched to series of two or three words, each of which started with a capital letter. This match is far from perfect and results in several false positives, and so probably can be refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(text):\n",
    "    pattern = '[A-Z]+[a-z]+$'\n",
    "    if re.search(pattern, text): \n",
    "        return(True) \n",
    "    else: \n",
    "        return(False)\n",
    "\n",
    "cit_author_2 = df['citation_general']\n",
    "\n",
    "newcits = []\n",
    "for item in cit_author_2:\n",
    "    try:\n",
    "        found = re.search('\\n(.+),', item).group(1)\n",
    "        found_lst = found.split()\n",
    "        if len(found_lst) == 2:\n",
    "            if match(found_lst[0]) and match(found_lst[1]):\n",
    "                newcits.append(found)\n",
    "        elif len(found_lst) == 3:\n",
    "            if match(found_lst[0]) and match(found_lst[1])and match(found_lst[2]):\n",
    "                newcits.append(found)\n",
    "  \n",
    "    except AttributeError:\n",
    "        found = ''\n",
    "\n",
    "#gets unique values\n",
    "newcits_set = set(newcits)\n",
    "newcits = list(newcits_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newcits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the list of article authors (x) with the list of names that we extracted (newcits) into a new dataframe (df1), and create a sorted version (df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templist = []\n",
    "combined_auth = x + newcits\n",
    "for i in combined_auth:\n",
    "    a = str(i)\n",
    "    templist.append(a)\n",
    "df1 = pd.DataFrame({'Author':templist})\n",
    "authseries = pd.Series(templist)\n",
    "authseries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.sort_values(\"Author\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is now saved in CSV form, so it can be manually cleaned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('sorted_citations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"sorted_citations.csv\" file was then manually cleaned using OpenRefine and Excel.  We had to strip some punctuation and delete the false positives, usually book titles.  For our particular data, we found one case where a list of author names was given a single id - in this case we broke apart the author names into separate lines and assigned new ids to each auhtor name.  The trickiest part was finding the same person represented in different ways (e.g., with or without a middle initial) and changing the id so that they all point to the same numerical id.  For the next part of the program, it is important not to delete these names - they are needed for the pattern matching that generates the citation vertices.\n",
    "\n",
    "Assuming that you closed out this program and cleaned your data, we start again by reloading the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('YOUR CITATIONS CSV FILE')\n",
    "df2 = pd.read_csv('sorted_citations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine goes through each citation noted in df, checks the citation_general to see if there is a match to one of the names in df2, and if there is creates a tuple of the index numbers of the article's author and the cited author.  \"Vertices\" is thus a list of tuples.  The tricky part here is the column indices and the returns from the iterrows method - if your dataframes are like ours (df2 has two columns, index number and name, and df has seven columns, with the article author in column 3 and citation_general in column 7) it should work. This routine takes a long time to run (about four hours for our data); there must be more efficient ways to do it.  Just to be safe, we create a saved copy when it ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuthId (auth_name):\n",
    "    for a, b in df2.iterrows():\n",
    "        authtocheck = b[1]\n",
    "        authartid = b[0]\n",
    "        if authtocheck == auth_name:\n",
    "            return authartid\n",
    "\n",
    "vertices = []\n",
    "for i, j in df.iterrows():\n",
    "    artid = j[0]\n",
    "    auth_name = j[2]\n",
    "    authid = getAuthId(auth_name)\n",
    "    citation = j[6]\n",
    "    \n",
    "    for k, l in df2.iterrows():\n",
    "        authindex = l[0]\n",
    "        authname = l[1]\n",
    "        if authname in citation:\n",
    "            vertices.append(tuple((authid,authindex)))\n",
    "            \n",
    "with open ('vertices.pickle','wb') as f:\n",
    "    pickle.dump(vertices,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the variable vertices is still active (you may need to reload the vertices.pickle file), we check to see that the list of tuples is of an appropriate length.  We then export the file as a csv.  Both vertices.csv and the sorted_citations.csv fils can then be exported into Gephi or used with the network analysis program of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vertices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('vertices.csv','w') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['source','target'])\n",
    "    for row in vertices:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}